---
title: "Apache AirFlow"
categories:
  - IT
tags:
  - Database
  - Extract, transform, load
  - Apache
last_modified_at: 2014-12-03T12:00:00-15:00
---

**[Airflow](https://airflow.apache.org/)** ([Github](https://github.com/apache/airflow)) is a platform created by the community to programmatically author, schedule and monitor workflows.

Use Airflow to author workflows as [Directed Acyclic Graphs (**DAGs**)](https://en.wikipedia.org/wiki/Directed_acyclic_graph) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.

## Directed acyclic graph

In mathematics, particularly graph theory, and computer science, a directed acyclic graph (DAG or dag) is a finite directed graph with no directed cycles. That is, it consists of finitely many vertices and edges (also called arcs), with each edge directed from one vertex to another, such that there is no way to start at any vertex v and follow a consistently-directed sequence of edges that eventually loops back to v again. Equivalently, a DAG is a directed graph that has a [topological ordering](https://en.wikipedia.org/wiki/Topological_sorting), a sequence of the vertices such that every edge is directed from earlier to later in the sequence. 

## Principles

- Dynamic: Airflow pipelines are configuration as code (Python), allowing for dynamic pipeline generation. This allows for writing code that instantiates pipelines dynamically.
- Extensible: Easily define your own operators, executors and extend the library so that it fits the level of abstraction that suits your environment.
- Elegant: Airflow pipelines are lean and explicit. Parameterizing your scripts is built into the core of Airflow using the powerful Jinja templating engine.
- Scalable: Airflow has a modular architecture and uses a message queue to orchestrate an arbitrary number of workers. Airflow is ready to scale to infinity.

## Installation

The easiest way to install the latest stable version of Airflow is with pip:

```bash
pip install apache-airflow
```

You can also install Airflow with support for extra features like gcp or postgres:

```bash
pip install apache-airflow[postgres,gcp]
```


## Initiating Airflow Database

Airflow requires a database to be initiated before you can run tasks. If you’re just experimenting and learning Airflow, you can stick with the default SQLite option. If you don’t want to use SQLite, then take a look at [Initializing a Database Backend](https://airflow.apache.org/docs/stable/howto/initialize-database.html) to setup a different database.

After configuration, you’ll need to initialize the database before you can run tasks:

```bash
airflow initdb
```

