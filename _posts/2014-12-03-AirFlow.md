---
title: "Apache AirFlow"
categories:
  - IT
tags:
  - Database
  - Extract, transform, load
  - Apache
last_modified_at: 2014-12-03T12:00:00-15:00
---

**[Airflow](https://airflow.apache.org/)** ([Github](https://github.com/apache/airflow)) is a platform created by the community to programmatically author, schedule and monitor workflows.

Use Airflow to author workflows as [Directed Acyclic Graphs (**DAGs**)](https://en.wikipedia.org/wiki/Directed_acyclic_graph) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.

## Directed acyclic graph

In mathematics, particularly graph theory, and computer science, a directed acyclic graph (DAG or dag) is a finite directed graph with no directed cycles. That is, it consists of finitely many vertices and edges (also called arcs), with each edge directed from one vertex to another, such that there is no way to start at any vertex v and follow a consistently-directed sequence of edges that eventually loops back to v again. Equivalently, a DAG is a directed graph that has a [topological ordering](https://en.wikipedia.org/wiki/Topological_sorting), a sequence of the vertices such that every edge is directed from earlier to later in the sequence. 

## Principles

- **Dynamic**: Airflow pipelines are configuration as code (Python), allowing for dynamic pipeline generation. This allows for writing code that instantiates pipelines dynamically.
- **Extensible**: Easily define your own operators, executors and extend the library so that it fits the level of abstraction that suits your environment.
- **Elegant**: Airflow pipelines are lean and explicit. Parameterizing your scripts is built into the core of Airflow using the powerful Jinja templating engine.
- **Scalable**: Airflow has a modular architecture and uses a message queue to orchestrate an arbitrary number of workers. Airflow is ready to scale to infinity.

## [Macros reference](https://airflow.apache.org/docs/stable/macros-ref.html)

Variables and macros can be used in templates (see the Jinja Templating section).

## [Jinja](https://jinja.palletsprojects.com/en/master/)

Jinja is a fast, expressive, extensible **templating engine**. Special placeholders in the template allow writing code similar to Python syntax. Then the template is passed data to render the final document.

## Installation

The easiest way to install the latest stable version of Airflow is with pip:

```bash
pip install apache-airflow
```

You can also install Airflow with support for extra features like gcp or postgres:

```bash
pip install apache-airflow[postgres,gcp]
```


## Initiating Airflow Database

Airflow requires a database to be initiated before you can run tasks. If you’re just experimenting and learning Airflow, you can stick with the default SQLite option. If you don’t want to use SQLite, then take a look at [Initializing a Database Backend](https://airflow.apache.org/docs/stable/howto/initialize-database.html) to setup a different database.

After configuration, you’ll need to initialize the database before you can run tasks:

```bash
airflow initdb
```

## Example

`airflow/example_dags/tutorial.py`

```python
# ############## #
# Import Modules #
# ############## #
from datetime import timedelta
# The DAG object; we'll need this to instantiate a DAG
from airflow import DAG
# Operators; we need this to operate!
from airflow.operators.bash_operator import BashOperator
from airflow.utils.dates import days_ago
```

```python
# ################# #
# Default Arguments #
# ################# #
# These args will get passed on to each operator
# You can override them on a per-task basis during operator initialization
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': days_ago(2),
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    # 'queue': 'bash_queue',
    # 'pool': 'backfill',
    # 'priority_weight': 10,
    # 'end_date': datetime(2016, 1, 1),
    # 'wait_for_downstream': False,
    # 'dag': dag,
    # 'sla': timedelta(hours=2),
    # 'execution_timeout': timedelta(seconds=300),
    # 'on_failure_callback': some_function,
    # 'on_success_callback': some_other_function,
    # 'on_retry_callback': another_function,
    # 'sla_miss_callback': yet_another_function,
    # 'trigger_rule': 'all_success'
}
```

```python
# ################# #
# Instantiate a DAG #
# ################# #
dag = DAG(
    'tutorial',
    default_args=default_args,
    description='A simple tutorial DAG',
    schedule_interval=timedelta(days=1),
)
```

```python
# ##### #
# Tasks #
# ##### #
# t1, t2 and t3 are examples of tasks created by instantiating operators
t1 = BashOperator(
    task_id='print_date',
    bash_command='date',
    dag=dag,
)

t2 = BashOperator(
    task_id='sleep',
    depends_on_past=False,
    bash_command='sleep 5',
    retries=3,
    dag=dag,
)
dag.doc_md = __doc__
```

```python
# ################### #
# Tasks Documentation #
# ################### #
t1.doc_md = """\
#### Task Documentation
You can document your task using the attributes `doc_md` (markdown),
`doc` (plain text), `doc_rst`, `doc_json`, `doc_yaml` which gets
rendered in the UI's Task Instance Details page.
![img](http://montcs.bloomu.edu/~bobmon/Semesters/2012-01/491/import%20soul.png)
"""
```

```python
# ##################### #
# Templating with Jinja #
# ##################### #
templated_command = """\
{% for i in range(5) %}
    echo "{{ ds }}"
    echo "{{ macros.ds_add(ds, 7)}}"
    echo "{{ params.my_param }}"
{% endfor %}
"""

# ##### #
# Tasks #
# ##### #
t3 = BashOperator(
    task_id='templated',
    depends_on_past=False,
    bash_command=templated_command,
    params={'my_param': 'Parameter I passed in'},
    dag=dag,
)
```

```python
# ####################### #
# Setting up Dependencies #
# ####################### #
# The bit shift operator can also be
# used to chain operations:
t1 >> [t2, t3]

# A list of tasks can also be set as
# dependencies. These operations
# all have the same effect:
# t1.set_downstream([t2, t3])
# t1 >> [t2, t3]
# [t2, t3] << t1
```